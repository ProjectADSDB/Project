# -*- coding: utf-8 -*-
"""Exploitation_zone.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TC9gNXR5aNxb6xHJ_eJlme9xNnSIrzHz

# Setup
"""

#from google.colab import drive
#drive.mount('/content/drive')

# Import necessary libraries
import os
import shutil


# Install and Setup PostgreSQL
import subprocess

# Replace !apt-get command
subprocess.run(["apt-get", "-y", "-qq", "install", "postgresql", "postgresql-contrib"], check=True)

# Replace !service command
subprocess.run(["service", "postgresql", "start"], check=True)

drive_path = "/content/drive/MyDrive/ADSDB/trusted/dumpfile.sql"
dump_path= "/content/dumpfile.sql"
# Check if the dumpfile exists in the Google Drive
if not os.path.exists(drive_path):
  print("Dump file not found in Google Drive.")


# Copy the dumpfile from Google Drive to Colab's environment
shutil.copy(drive_path, dump_path)
# Create a temporary .pgpass file for authentication
with open("/root/.pgpass", "w") as f:
  f.write("*:*:*:adsdb:adsdb")
os.chmod("/root/.pgpass", 0o600)  # Set the required permissions

# Restore the database using psql command
try:
  subprocess.run(['PGPASSFILE=/root/.pgpass', 'psql', '-h', 'localhost', '-U', 'adsdb', '-d', 'adsdb', '-f', dump_path], shell=True)
  print("Restoration successful!")
except Exception as e:
  print(f"Error during restoration: {e}")

cmd = 'PGPASSFILE=/root/.pgpass psql -h localhost -U adsdb -d adsdb -c "SELECT table_name FROM information_schema.tables WHERE table_schema=\'public\';"'
subprocess.run(cmd, shell=True)


def show_table(table_name):
  display_query = f"SELECT * FROM {table_name} LIMIT 20;"
  cmd = f'PGPASSFILE=/root/.pgpass psql -h localhost -U adsdb -d adsdb -c "{display_query}"'
  subprocess.run(cmd, shell=True)

def get_table_length(table_name):
  count_query = f"SELECT COUNT(*) FROM {table_name};"
  cmd = f'PGPASSFILE=/root/.pgpass psql -h localhost -U adsdb -d adsdb -c "{count_query}"'
  subprocess.run(cmd, shell=True)

show_table('final_house_price_index')
get_table_length('final_house_price_index')

"""# Merge Quarters"""

# aggregate data for each year in the tables final_employment_rate and final_house_price_index

# 3 modalities: take the mean, take the last quarter, take the first  'mean','q1','q4'
modality = 'mean'

def merge_quarters(table_name, modality):
  if (modality=='q1'):
    merge_query = f"""
    SELECT *
    FROM {table_name}
    WHERE quarter = 1;
    """
  elif (modality=='q4'):
    merge_query = f"""
    SELECT *
    FROM {table_name}
    WHERE quarter = 4
    """
  else:
    if (table_name == 'final_employment_rate'):
      merge_query = f"""
      SELECT year, sex, provinces, rates, AVG(total) as total
      FROM {table_name}
      GROUP BY year, sex, provinces, rates
      ORDER BY year, sex, provinces, rates;
      """
    elif (table_name == 'final_house_price_index'):
      merge_query = f"""
      SELECT year, indices_and_rates, index_type, autonomous_communities_and_cities, national_total, AVG(total) as total
      FROM {table_name}
      GROUP BY year, indices_and_rates, index_type, autonomous_communities_and_cities, national_total
      ORDER BY year, indices_and_rates, index_type, autonomous_communities_and_cities, national_total;
      """

  # Define the name for the new table
  new_table_name = 'merged_' + table_name

  check_table_query = f"""
      SELECT to_regclass('public.{new_table_name}') IS NOT NULL;
      """
  # Run the query to check if the table exists
  cmd = f'PGPASSFILE=/root/.pgpass psql -h localhost -U adsdb -d adsdb -c "{check_table_query}"'
  subprocess.run(cmd, shell=True)
  table_exists = check_result[0].strip() == 't'


  # If the table exists, drop the old one
  if table_exists:
     print('dropping table')
     drop_query = f"""
     DROP TABLE {new_table_name};
     """
     cmd = f'PGPASSFILE=/root/.pgpass psql -h localhost -U adsdb -d adsdb -c "{drop_query}"'
     subprocess.run(cmd, shell=True)

  # Create the new table with the same structure as the query result
  create_table_query = f"""
  CREATE TABLE {new_table_name} AS {merge_query};
  """
  cmd = f'PGPASSFILE=/root/.pgpass psql -h localhost -U adsdb -d adsdb -c "{create_table_query}"'
  subprocess.run(cmd, shell=True)


modality = 'mean'
merge_quarters('final_employment_rate', modality)
merge_quarters('final_house_price_index', modality)

show_table('merged_final_employment_rate')
get_table_length('merged_final_employment_rate')

# MERGE PROVINCES INTO COMMUNITIES IN employment_rate
provinces_to_communities = {
    '01 Andalucía': ['02 Albacete', '03 Alicante/Alacant', '04 Almería', '11 Cádiz', '14 Córdoba', '18 Granada', '23 Jaén', '29 Málaga', '41 Sevilla'],
    '02 Aragón': ['50 Zaragoza', '44 Teruel', '22 Huesca'],
    '03 Asturias. Principado de': ['33 Asturias'],
    '04 Balears. Illes': ['07 Balears. Illes'],
    '05 Canarias': ['38 Santa Cruz de Tenerife', '35 Palmas. Las'],
    '06 Cantabria': ['39 Cantabria'],
    '07 Castilla y León': ['05 Ávila', '09 Burgos', '24 León', '34 Palencia', '37 Salamanca', '40 Segovia', '42 Soria', '47 Valladolid', '49 Zamora'],
    '08 Castilla - La Mancha': ['02 Albacete', '13 Ciudad Real', '16 Cuenca', '19 Guadalajara', '45 Toledo'],
    '09 Cataluña': ['08 Barcelona', '17 Girona', '25 Lleida', '43 Tarragona'],
    '10 Comunitat Valenciana': ['03 Alicante/Alacant', '12 Castellón/Castelló', '46 Valencia/València'],
    '11 Extremadura': ['06 Badajoz', '10 Cáceres'],
    '12 Galicia': ['15 Coruña. A', '27 Lugo', '32 Ourense', '36 Pontevedra'],
    '13 Madrid. Comunidad de': ['28 Madrid'],
    '14 Murcia. Región de': ['30 Murcia'],
    '15 Navarra. Comunidad Foral de': ['31 Navarra'],
    '16 País Vasco': ['48 Bizkaia', '20 Gipuzkoa'],
    '17 Rioja. La': ['26 Rioja. La'],
    '18 Ceuta': ['51 Ceuta'],
    '19 Melilla': ['52 Melilla'],
    'NaN': ['National Total'],
}

def merge_provinces(table_name):
    for community, provinces in provinces_to_communities.items():
        for province in provinces:
            # Create SQL UPDATE query to update 'provinces' to 'community' where 'provinces' is 'province'
            merge_query = f"UPDATE {table_name} SET provinces = '{community}' WHERE provinces = '{province}'"
            cmd = f'PGPASSFILE=/root/.pgpass psql -h localhost -U adsdb -d adsdb -c "{merge_query}"'
            subprocess.run(cmd, shell=True)

    merge_query = f"""
      SELECT year, sex, provinces as communities, rates, AVG(total) as total
      FROM {table_name}
      GROUP BY year, sex, communities, rates
      ORDER BY year, sex, communities, rates;
      """
    new_table_name = 'merged_final_employment_rate2'
    create_table_query = f"""
    CREATE TABLE {new_table_name} AS {merge_query};
    """
    cmd = f'PGPASSFILE=/root/.pgpass psql -h localhost -U adsdb -d adsdb -c "{create_table_query}"'
    subprocess.run(cmd, shell=True)

merge_provinces('merged_final_employment_rate')
get_table_length('merged_final_employment_rate')
get_table_length('merged_final_employment_rate2')

cmd = f'PGPASSFILE=/root/.pgpass psql -h localhost -U adsdb -d adsdb -c "SELECT table_name FROM information_schema.tables WHERE table_schema=\'public\';"'
subprocess.run(cmd, shell=True)

#omogenize table names
drop_query = f"""
     DROP TABLE IF EXISTS final_employment_rate, final_house_price_index, merged_final_employment_rate;
     """
cmd = f'PGPASSFILE=/root/.pgpass psql -h localhost -U adsdb -d adsdb -c "{drop_query}"'
subprocess.run(cmd, shell=True)

change_query = f"""
     ALTER TABLE merged_final_house_price_index RENAME TO final_house_price_index;
     """
cmd = f'PGPASSFILE=/root/.pgpass psql -h localhost -U adsdb -d adsdb -c "{change_query}"'
subprocess.run(cmd, shell=True)

change_query = f"""
     ALTER TABLE merged_final_employment_rate2 RENAME TO final_employment_rate;
     """
cmd = f'PGPASSFILE=/root/.pgpass psql -h localhost -U adsdb -d adsdb -c "{change_query}"'
subprocess.run(cmd, shell=True)

"""# Drop Columns"""

cmd = f'PGPASSFILE=/root/.pgpass psql -h localhost -U adsdb -d adsdb -c "SELECT table_name FROM information_schema.tables WHERE table_schema=\'public\';"'
subprocess.run(cmd, shell=True)

import subprocess

def execute_sql_command(sql_command):
  query = f"{sql_command}"
  print(query)
  cmd = f'PGPASSFILE=/root/.pgpass psql -h localhost -U adsdb -d adsdb -c "{query}"'
  subprocess.run(cmd, shell=True)

# Example usage:
sql_command = "SELECT * FROM final_employment_rate LIMIT 20;"
execute_sql_command(sql_command)

# EMPLOYMENT_RATE
# DROP RWOS WITH BOTH SEX

execute_sql_command("DELETE FROM final_employment_rate WHERE sex = 'Both sexes';")
execute_sql_command("SELECT * FROM final_employment_rate LIMIT 20;")
get_table_length("final_employment_rate")

# HOUSE_PRICE_INDEX
# Drop indices_and_rates different from 'Index' (annual and quarter variation)
execute_sql_command("DELETE FROM final_house_price_index WHERE indices_and_rates <> 'Index';")
# Drop column nationaò_total
execute_sql_command("ALTER TABLE final_house_price_index DROP COLUMN national_total;")
# NaN values in autonomous communities and cities become 'National'
execute_sql_command("UPDATE final_house_price_index SET autonomous_communities_and_cities = 'National' WHERE autonomous_communities_and_cities = 'NaN';")

execute_sql_command("SELECT * FROM final_house_price_index LIMIT 20;")
get_table_length("final_house_price_index")

# HOUSE PRICE INDEX WEIGHTS

"""# Join"""

# JOIN of inflation_rate x house_price_index
execute_sql_command("""CREATE TABLE JoinedTable AS
SELECT final_inflation_rate.*, price.indices_and_rates, price.index_type, price.autonomous_communities_and_cities, price.total
FROM final_inflation_rate
INNER JOIN final_house_price_index price ON final_inflation_rate.year = price.year;
""")
cmd = f'PGPASSFILE=/root/.pgpass psql -h localhost -U adsdb -d adsdb -c "SELECT table_name FROM information_schema.tables WHERE table_schema=\'public\';"'
subprocess.run(cmd, shell=True)

show_table('joinedtable')
get_table_length('joinedtable')

# JOIN x employment_rate
show_table('final_employment_rate')

cmd = f'PGPASSFILE=/root/.pgpass psql -h localhost -U adsdb -d adsdb -c "SELECT table_name FROM information_schema.tables WHERE table_schema=\'public\';"'
subprocess.run(cmd, shell=True)

execute_sql_command("""CREATE TABLE JoinedTableFinal AS
SELECT joinedtable.*, e.sex, e.rates, e.total as employment_total
FROM joinedtable
INNER JOIN final_employment_rate e ON joinedtable.year = e.year AND joinedtable.autonomous_communities_and_cities = e.communities;
""")
show_table('JoinedTableFinal')
get_table_length('JoinedTableFinal')

"""# Data Analysis"""

import psycopg2
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import statsmodels.api as sm
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score


# Define PostgreSQL connection parameters
db_params = {
    'dbname': 'adsdb',
    'user': 'adsdb',
    'password': 'adsdb',
    'host': 'localhost',
    'port': '5432'
}

# Connect to the PostgreSQL database
conn = psycopg2.connect(**db_params)

# Read the 'JoinedTableFinal' into a DataFrame
df = pd.read_sql('SELECT * FROM JoinedTableFinal', conn)

# Close the database connection
conn.close()

df.head()

df.info()

# Get unique values
index_types = df['index_type'].unique()
rates_types = df['rates'].unique()

# Create an empty DataFrame for storing correlations
correlation_matrix = pd.DataFrame(index=index_types, columns=rates_types)

# Fill the DataFrame with correlations
for i_type in index_types:
    for r_type in rates_types:
        subset_df = df[(df['index_type'] == i_type) & (df['rates'] == r_type)]
        corr_total = subset_df['total'].corr(subset_df['inflation_rate_percent'])
        corr_employment = subset_df['employment_total'].corr(subset_df['inflation_rate_percent'])
        # Storing average of the two correlations for simplicity
        correlation_matrix.loc[i_type, r_type] = np.mean([corr_total, corr_employment])

# Convert to float for visualization
correlation_matrix = correlation_matrix.astype(float)

# Visualize the correlation matrix
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)
plt.title('Correlation Matrix for Index Types vs Rates')
plt.show()

"""## Linear regression"""

columns_to_plot = ['inflation_rate_percent', 'annual_change', 'employment_total']

def show_outliers(df, columns_to_plot):
    for col in columns_to_plot:
        # Create a separate boxplot for each column
        plt.figure(figsize=(10, 6))
        boxplot = df.boxplot(column=col)
        plt.title(f'Boxplot of {col}')
        plt.ylabel('Value')
        plt.grid(axis='y', linestyle='--', alpha=0.7)

        # Identify and output rows with outliers for the current column
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        # Find rows with outliers
        outlier_rows = df[(df[col] < lower_bound) | (df[col] > upper_bound)]

        # Print rows with outliers for the current column
        if not outlier_rows.empty:
            elements = list(outlier_rows['year'].unique())
            print(f"Rows with outliers in '{col}':", ", ".join(map(str, elements)), "\n")

        # Show the plot for the current column
        plt.show()


show_outliers(df, columns_to_plot)

import seaborn as sns
import matplotlib.pyplot as plt
import statsmodels.api as sm
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# 1. Data Pre-processing
# Convert categorical columns to numerical
mean_value = df['total'].mean()
df['total'].fillna(mean_value, inplace=True)

df_joined_table_final = pd.get_dummies(df, columns=['index_type', 'autonomous_communities_and_cities', 'sex', 'rates'], drop_first=True)

# 2. Correlation Analysis
correlation_matrix = df_joined_table_final.corr()
plt.figure(figsize=(20,20))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title("Correlation Heatmap")
plt.show()

# 3. Regression Analysis
# Target variable is 'total' (house price index)
# Features will include inflation and labor market variables
X = df_joined_table_final[['inflation_rate_percent', 'annual_change', 'employment_total'] + [col for col in df_joined_table_final if 'index_type_' in col or 'sex_' in col or 'rates_' in col]]
y = df_joined_table_final['total']

# Splitting data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Fit a linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)

# Model Evaluation
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f"Mean Squared Error: {mse}")
print(f"R^2 Score: {r2}")

# 4. Residual Analysis
residuals = y_test - y_pred
plt.figure(figsize=(10,6))
sns.distplot(residuals)
plt.title("Residuals Distribution")
plt.show()

# Scatter plot of residuals
plt.figure(figsize=(10,6))
sns.scatterplot(x=y_test, y=residuals)
plt.axhline(0, color='red', linestyle='--')
plt.title("Scatter plot of residuals")
plt.show()

"""From **Mean Squared Error: `281.60636094576813`** and **R^2 Score: `0.20390947397994785`** we can tell a linear regression model is not able to capture enough information for our dataset. We will try to implement more advanced model now.

## Random forest
"""

# Import necessary libraries
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Feature scaling
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Initialize and train the Random Forest Regressor
rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# Make predictions
y_pred = rf.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f"Mean Squared Error (MSE): {mse}")
print(f"R-squared (R2): {r2}")

# If you want to see feature importance
feature_importances = rf.feature_importances_
features = list(X.columns)
for feature, importance in zip(features, feature_importances):
    print(f"Feature: {feature}, Importance: {importance}")

"""*   **R-squared (R2)**: approximately `0.7787`. This means that around `77.87%` of the variability in our target variable can be explained by the features in our model. This is a significant improvement compared to the linear regression model we tried earlier.


*   annual_change: This is the most important feature in your model, contributing to approximately `45.79%` of the prediction.
*   inflation_rate_percent: The second most important feature, accounting for about `26.83%`.
*   employment_total: The third most influential feature, with an importance of approximately `20.50%`.

## SVR
"""

# Import necessary libraries
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score

# Initialize and train the Random Forest Regressor
svr = SVR(kernel='linear')
svr.fit(X_train, y_train)

# Make predictions
y_pred = svr.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f"Mean Squared Error (MSE): {mse}")
print(f"R-squared (R2): {r2}")

"""## XGB Regressor"""

import xgboost as xgb
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score

# Initialize and train the Random Forest Regressor
xgb_reg = xgb.XGBRegressor()
xgb_reg.fit(X_train, y_train)

# Make predictions
y_pred = xgb_reg.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f"Mean Squared Error (MSE): {mse}")
print(f"R-squared (R2): {r2}")

# If you want to see feature importance
feature_importances = xgb_reg.feature_importances_
features = list(X.columns)
for feature, importance in zip(features, feature_importances):
    print(f"Feature: {feature}, Importance: {importance}")

"""*   **R-squared (R2)**: approximately `0.8343`. This means that around `83.43%` of the variability in our target variable can be explained by the features in our model. This information combined with the lowest MSE score of `58.61` indicates that it is the best model so far."""