# -*- coding: utf-8 -*-
"""dataprofiling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qLVts7x4YzfkQn8hpC67EdoWGCONvc39
"""

#from google.colab import drive
#drive.mount('/content/drive')

import os
import shutil

import subprocess

# Replace !apt-get command
subprocess.run(["apt-get", "-y", "-qq", "install", "postgresql", "postgresql-contrib"], check=True)

# Replace !service command
subprocess.run(["service", "postgresql", "start"], check=True)

# Creating a superuser named adsdb
#try:
#    subprocess.run(["sudo", "-u", "postgres", "createuser", "--superuser", "adsdb"], check=True)
#except subprocess.CalledProcessError:
#    print("The user 'adsdb' already exists.")

# Creating a database named adsdb
#subprocess.run(["sudo", "-u", "postgres", "createdb", "adsdb"], check=True)

# Setting the password for the user adsdb to 'adsdb'
#subprocess.run(["sudo", "-u", "postgres", "psql", "-c", "ALTER USER adsdb WITH PASSWORD 'adsdb';"], check=True)

drive_path = "/content/drive/MyDrive/ADSDB/trusted/dumpfile.sql"
dump_path= "/content/dumpfile.sql"
# Check if the dumpfile exists in the Google Drive
if not os.path.exists(drive_path):
  print("Dump file not found in Google Drive.")


# Copy the dumpfile from Google Drive to Colab's environment
shutil.copy(drive_path, dump_path)
# Create a temporary .pgpass file for authentication
with open("/root/.pgpass", "w") as f:
  f.write("*:*:*:adsdb:adsdb")
os.chmod("/root/.pgpass", 0o600)  # Set the required permissions

import psycopg2
import pandas as pd

# Create a dictionary to store DataFrames and lengths for each table
dataframes = {}

try:
    subprocess.run(['PGPASSFILE=/root/.pgpass', 'pg_dump', '-h', 'localhost', '-U', 'adsdb', '-d', 'adsdb', '-f', dump_path], shell=True)
    print("Restoration successful!")

    # Connect to the restored database
    conn = psycopg2.connect(database="adsdb", user="adsdb", host="localhost", password="adsdb")

    # Create a cursor object to interact with the database
    cur = conn.cursor()

    # Query the list of tables in the database
    cur.execute("SELECT table_name FROM information_schema.tables WHERE table_schema = 'public'")

    # Fetch all table names
    tables = cur.fetchall()

    if tables:
        for table in tables:
            table_name = table[0]

            # Fetch both column names and data in a single query
            cur.execute(f"SELECT * FROM {table_name}")
            table_data = cur.fetchall()
            column_names = [desc[0] for desc in cur.description]

            # Create a Pandas DataFrame from the fetched data and column names
            df = pd.DataFrame(table_data, columns=column_names)

            # Save the DataFrame in the dataframes dictionary
            dataframes[table_name] = df

            # Get the length (number of rows) of the DataFrame
            num_rows = len(df)

            # Print the table name, number of rows, and the first few rows of the DataFrame
            print(f"Table: {table_name} | Number of Rows: {num_rows}")
            print(df.head())

    else:
        print("No tables found in the database.")

    # Close the cursor and connection
    cur.close()
    conn.close()

except Exception as e:
    print(f"Error during restoration: {e}")

#df_house_i= dataframes['final_house_price_index']
#df_house_w = dataframes['final_house_price_index_weights']
#df_employment = dataframes['final_employment_rate']
#df_employment.head()
#print(df['year'])

def data_profile(df):
  # 1. Basic Statistics
  if 'total' in df.columns:
    df['total'] = pd.to_numeric(df['total'], errors='coerce')
  summary_stats = df.describe()
  print(summary_stats)
  print('-' * 40)

  # 2. Data Type Assessment
  data_types = df.dtypes
  print(data_types)
  print('-' * 40)

  # 3. Missing Value Analysis
  missing_values = df.isnull().sum()
  print(missing_values)
  nan_counts = df.isna().sum()
  print(nan_counts)
  print('-' * 40)

#data_profile(df_employment)
